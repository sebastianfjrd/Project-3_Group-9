{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18169,
     "status": "ok",
     "timestamp": 1723767015061,
     "user": {
      "displayName": "ghosalli@ymail.com",
      "userId": "06654975647961110478"
     },
     "user_tz": 240
    },
    "id": "U51HEpt3PQEL",
    "outputId": "bb77b74e-88b1-4f99-fb0c-14fcbc177fe7"
   },
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model # To load the model later to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 01:05:42.397337: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-08-17 01:05:42.397389: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-08-17 01:05:42.397419: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-08-17 01:05:42.397451: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-17 01:05:42.397553: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/anaconda3/envs/ML_DEV/lib/python3.10/site-packages/keras/src/trainers/trainer.py:210: UserWarning: Model doesn't support `jit_compile=True`. Proceeding with `jit_compile=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('saved_model/melanoma_cnn_model.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert Image file to numpy file, convert image to floating point & normalize the file and create image_data\n",
    "def format_image_for_prediction(image):\n",
    "    # Resize image to 300x300 \n",
    "    image.resize((300,300))\n",
    "    # Normalize image\n",
    "    image_normalized = np.array(image).astype(np.float32) / 255\n",
    "    # Convert values to numpy arrays\n",
    "    np.array(image_normalized)\n",
    "    # Add batch dimension\n",
    "    image_normalized = np.expand_dims(image_normalized, axis=0)  # Shape becomes (1, 300, 300, 3)\n",
    "    \n",
    "    return image_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(image_normalized):\n",
    "    # # Load the model\n",
    "    # model = load_model('saved_model/melanoma_cnn_model.keras')\n",
    "    \n",
    "    # Predict\n",
    "    prediction_probs = model.predict(image_normalized)\n",
    "    prediction = np.argmax(prediction_probs, axis=1)\n",
    "    \n",
    "    # Interpret the prediction\n",
    "    if prediction == 1:\n",
    "        prediction = \"Our model's prediction is malignant.\"\n",
    "    else:\n",
    "        prediction = \"Our model's prediction is benign.\"\n",
    "\n",
    "    return prediction\n",
    "    # Here you can add code to process the image\n",
    "    #return image  # Returning the same image for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    # Resize and normalize and convert image to numpy array\n",
    "    image_normalized = format_image_for_prediction(image)\n",
    "    # Get prediction\n",
    "    return get_prediction(image_normalized)\n",
    "    \n",
    "    # return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 11663,
     "status": "ok",
     "timestamp": 1723767028870,
     "user": {
      "displayName": "ghosalli@ymail.com",
      "userId": "06654975647961110478"
     },
     "user_tz": 240
    },
    "id": "TBc5w7sFPFMl",
    "outputId": "72356ccb-911c-4423-9f6a-e11009432faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "Running on public URL: https://ef1be49e885985bd6f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ef1be49e885985bd6f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Gradio interface with an image upload button\n",
    "my_demo = gr.Interface(\n",
    "    fn=process_image,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload an Image\"),  \n",
    "    outputs=\"text\"\n",
    "  )\n",
    "\n",
    "my_demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9NIuxeGPKcI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrBdjmWVMsMQtURMJ0ybhZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
